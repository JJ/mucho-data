<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Data is out there</title>

		<meta name="description" content="Big data, presentation for Tom's machine learning class">
		<meta name="author" content="JJ Merelo">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/league.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

	  <div class="reveal">
	    
	    <!-- Any section element inside of this container is displayed as a slide -->
	    <div class="slides">
	      <section>
		<h1 class='big'><em>Carpe</em> data</h1>

		<aside class='notes'>"Seize" the data</aside>
	      </section>


	      <section data-background='https://farm9.staticflickr.com/8322/8031897271_47737e43b1_k_d.jpg'>
		<aside class='notes'>There's a lot of data out there, if only we knew how to grab it.</aside>
	      </section>

	      <section> <!-- APIS --> 
		<section data-background='https://farm8.staticflickr.com/7024/6510010063_2cc839f323_b_d.jpg' title='Apis mellifera'>

		  <h1 class='img'>Grab the API</h1>

		  <aside class='notes'><p>Los denominados
		  APIs, o interfaces de programación de
		  aplicaciones (<em>application
		    programming interface</em>), permiten extraer
		  información ya procesada sobre la
		  actividad online en una serie de
		  servicios en la web. Por ejemplo, los APIs permiten
		  recuperar tweets en Twitter (es lo
		  que las apps de Twitter como
		  Tweetdeck o Hootsuite), likes
		  en Facebook, visitas en la Wikipedia
		  y una gran cantidad de cosas sobre
		  la actividad pública de la
		  gente. También se puede acceder a
		  otro tipo de APIs, recordad que el
		  big data no se refiere sólo y exclusivamente a la gente:
		  por
		  ejemplo <a href='http://en.wikipedia.org/wiki/List_of_financial_data_feeds'>datos
		    de acciones en bolsa</a>
		  o <a href='https://code.google.com/p/googletransitdatafeed/wiki/PublicFeeds'>datos
		    de tráfico</a>
		  o <a href='http://stats.stackexchange.com/questions/12670/data-apis-feeds-available-as-packages-in-r'>datos
		    de todo tipo, desde el tiempo
		    hasta archivos</a>. Por cierto,
		  vemos por primera vez <a href='http://r-project.org'>R</a>, el lenguaje
		  que se usa, por excelencia, para
		  trabajar con los datos. Lo veremos
		  más adelante.</p>

		    <p>Curiously enough, there's a place
		    called <a href='http://api.is'><code>api.is</code></a>
		    which is a collection of scraped data from the web
		      in Iceland offered in a REST API</p>

		  </aside>

		</section>


		<section>
		  <pre><code>library(RCurl)
library(RJSONIO) #more stuff
for (year in (2010:2014)) {
    for (month in (1:12)) {        
        theURL <- "http://stats.grok.se/json/is/"
	theURL <- paste0(theURL, year)        
	theURL <- paste0(theURL, month) # Some month magic here 
	theURL.HI <- paste0(theURL, "/Háskóli_Íslands")
	rawData.HI <- getURL(theURL.HI)
	data.HI <- fromJSON(rawData.HI)
	df.HI <- data.frame(Date=names(data.HI$daily_views),Views=data.HI$daily_views)
	df.HI <-  df.HI[df.HI$Views > 0,]
	df.HI$Date <- as.Date(df.HI$Date)
	alldata.HI <- rbind( alldata.HI, df.HI ) # And more
    }
}</code></pre>

		      <aside class='notes'>Es sólo parte
		    del programa, que se puede descargar <a
							    href='datos/univs-is.R'>de
		      aquí</a>; también se puede
		    consultar la historia de R, es
		    decir las pruebas, los datos y el resto de los comandos
		    ejecutados. Recordad, todo software libre. En
		    cualquier caso, si no entendéis
		    del tema no tenéis que
		    preocuparos: está escrito en un
		    lenguaje llamado R, que es un
		    lenguaje que os vais a encontrar
		    una y otra vez cuando se habla de
		    ciencia de datos y de big
		    data. La idea es que los APIs o
		    interfaces de programación
		    <em>publican</em> los datos en un
		    formato estándar, que se pueden
		    usar desde cualquier lenguaje de
		    programación, en particular este
		    R. </aside>
		</section>

		<section>
		  <img src='img/hr-vs-hi-wp.png' alt='Wikipedia visits HR vs  HI'>
		  <aside class='notes'><p>Por ejemplo,
		      usando un API muy simple de la
		      Wikipedia se pueden descargar las
		      visitas a las páginas día a
		      día. Aquí las visitas a la página
		      de la Universidad de Alicante y la
		      de la Miguel Hernández, por
		      ejemplo.</p>
		    
		    <p>De hecho, la Wikipedia da
		      acceso a <a
				  href='https://wikitech.wikimedia.org/wiki/Analytics'>una
			buena cantidad de datos de una forma
			sencilla</a>. Viene a ser una de las
		      banderas de la nueva web (que ya no
		      sé si va por la dos o la tres o ya
		      ni siquiera se llama web): se crean
		      plataformas, no sólo sitios con
		      contenido. También es un signo de
		      transparencia: por eso se puede
		      acceder de esa forma a los datos de
		      la Wikipedia (y no tanto a los de
		      FB, por ejemplo).</p></aside>
		</section>
				  
		<section><h1>Throttling</h1></section>

		<section><h1>2 step authentication: <code>OAuth</code></h1></section>
	      </section> <!-- APIs -->

	      <section> <!-- Scraping -->
				  
		<section data-background='https://farm8.staticflickr.com/7467/15897351745_ccc1232f4e_h_d.jpg' title='Rascando/scraping'>
		  <aside class='notes'>Pero no todo
		    el mundo es tan fácil y
		    accesible. <em>Scraping</em> es
		    extraer datos de dónde estén
		    publicados sin, en principio,
		    intención de que se descarguen. 
		  </aside>
		</section>

		<section>
		  <h2>Pepsi-Deildin table</h2>
		  <img src='img/pepsi-deildin-table.png'
		       alt='Classification'>
		  
		  <aside class='notes'>Data on the web usually
		  appears in tables; in that case, it's quite easy to
		  capture it (in fact, you can do it via copy/paste).</aside>
		</section>
		
		<section>
		  <h2>From there to here</h2>
		  <img src='img/stig-vs-net-is.png' alt='Stig vs net
		  whatever that means'>

		  <aside class='notes'>We are maybe interested in
		  creating a model on how goal average is related to
		    score and how this is similar to other European
		  leagues</aside>
		</section>
		<section>
		  <pre><code>my $url = "http://www.ksi.is/mot/motalisti/urslit-stada/?MotNumer=33503";

my $dom = Mojo::DOM->new( get $url );

my $classification_data =  $dom->find("table#stodu-tafla tr");

my %classification;

shift @$classification_data; # off with the headers

my %data = ( u => 3,
	     j => 4,
	     t => 5,
	     net => 7,
	     stig => 8 );
</code></pre>

				    <aside class='notes'>This program
				    uses Perl, which is excellent for
				    many different reasons, including
				    the availability of libraries for
				      scraping. We're
				    using <code>Mojo::DOM</code> here.</aside>
				  </section>

				<section><pre><code>
for my $p ( @$classification_data ) {
  my $rows = $p->find( "td" );
  my $team = $rows->[1]->all_text();
  $classification{$team} = {}; 
  for my $d ( keys %data ) {
    $classification{$team}->{$d} = $rows->[$data{$d}]->text();
  }
}
say to_json  \%classification ;</code></pre>

<aside class='notes'>This navigates the table to extract info from it</aside>
</section>

<section>
  <h2>Beautiful soup, hPricot, Cheerio</h2>
  </h2>
</section>

<section>
  <h1>Surf the data</h1>

  <aside class='notes'>Sometimes it's quite difficult to get to where
  the data is. Maybe it's generated on the fly, for instance in
  Hagstofa. But you can always navigate to data in the same way that
    you would: get to the page and click on the buttons.</aside>
</section>

<section>
  <img src='img/hagstofa.png' alt='selecting stuff from Statistice'>

  <aside class='notes'>You have to use this kind of thing to APIfy a
  website. All websites with forms actually have a REST interface you
  can "play" from outside. In some cases you can figure out what's
    needed, not in other cases, so you'll have to navigate the
  site. </aside>
</section>

<section>
  <pre><code>use WWW::Mechanize;
my $url = "https://rannsokn.hagstofa.is/pxen/Dialog/varval.asp?ma=SAM08005&ti=Number%20of%20trips%20and%20overnight%20by%20forms%20of%20tourism%20and%20classes%20of%20visitors,%202009-2013%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&path=../Database/ferdamal/ferdaidnadur/&lang=1&units=Trips%20or%20overnights";
my $mech = WWW::Mechanize->new();
$mech->get( $url );
$mech->select('values1','2');
$mech->select('values2', ['1','2','3','4','5']);
$mech->select('values3', '1');
$mech->select('values4', '1');
my $first_response = $mech->submit_form();
if ($first_response->is_success) {
  print $first_response->decoded_content;
}
else {
  print STDERR $response->status_line, "\n";
} #Much more stuff here
  </code></pre>

  <aside class='notes'>I couldn't make this work, but it would be an
    interesting exercise to do it.</aside>
</section>


<section>
  <pre><code>library("pxR")
library("ggplot2")
tourism.is<- read.px( "tourism-is.px" )
tourism.is.data   <-  as.data.frame( tourism.is )
tourism.is.data$Year <- as.integer(tourism.is.data$Year)
tourism.is.data.domestic <- tourism.is.data[tourism.is.data$Type.of.tourism=="Inbound tourism",]
domestic.trips <- tourism.is.data.domestic[tourism.is.data.domestic$Activity=="Number of trips",]
ggplot(data=domestic.trips, aes(x=Year,y=value))+geom_line() </code></pre>

		  <aside class='notes'>And this is way to read formats
		  such as PX, which is not proprietary, but it's the
		    one used here.</aside>
</section>

<section>
  <h3>The beautiful result</h3>
  <img src='img/tourists.png' alt='Amount of crap tourists are bringing
  to Iceland'>
</section>

				  <section>
				    <h1>Scraping where it itches</h1>
				    <aside class='notes'>Hay
				      herramientas para trabajar con
				      diferentes conjuntos de datos
				      automáticamente e incluso un sitio,
				      <a
					 href='http://morph.io'><code>morph.io</code></a>
				      para almacenar procedimientos de
				  scrapeado y ejecutarlos
				      automáticamente; todo ello en
				      scripts también libres que puedes
				      modificar y adaptar a las
				      necesidades propias. También hay <a
									  href='http://michelleminkoff.com/outwit-needlebase-hands-on-lab/'>extensiones
					a Firefox como Outwit</a>. Hay
				      que trabajar de <a
							 href='http://en.wikipedia.org/wiki/Web_scraping'>múltiples
					formas</a> pero, en muchos casos, se
				      pueden obtener datos suficientes
				      como para trabajar. Para hacer
				      scraping en profundidad, en general,
				      hay que tener ciertos conocimientos
				      de programación web y saber, al
				      menos, leer la estructura del fuente
				  de una página web, porque lo que se
				      hace, en la mayor parte de las
				      ocasiones, es eso. En algunos casos
				      también es cuestión de copiar y
				      pegar, pero si tienes que actualizar
				      la información cada cierto tiempo
				      puede acabar siendo muy tedioso. Más
				      recursos, incluyendo <a
							      href='https://zenagiwa.wordpress.com/2014/10/11/non-programmers-guide-to-scraping-data/'>extensiones
					para Chrome, están en esta
					página</a>. Incluso si usáis <a
									href='http://www.labnol.org/internet/google-web-scraping/28450/'>la
					hoja de cálculo de Google Drive y
					con un poquito de programación
					puedes trabajar hasta con cosas que
					se actualicen periódicamente.</a></aside>
				  </section>

				  <section>
				    <h1>Phantom.JS</h1>
				  </section>

				  <section>
				    <pre><code>#!/usr/bin/env perl
use strict;
use warnings;
use LWP::Simple;
use v5.14;
my $url= shift || "http://www.althingi.is/altext/cv/en/?nfaerslunr=97";
my $data = get($url);
my ($name) = ($data =~ m{<h2 class='boxhead'>([^<]+)</h2>});
my ($birth_month, $birth_year ) = ( $data =~ m{Date of Birth:</strong> (\w+).+(\d+)} );
my ($party) =  ( $data =~ m{strong>Party:</strong> ([^<]+)</li>} );
say "=> $name, $birth_month, $birth_year, $party"; </code></pre></section>



				<section style="text-align: left;">
					<h1>Questions?</h1>
				</section>

				 <section>
				   <section>
				     <h2>Image credits</h2>
				     
				     <ol>
				 	       
				       <li class='credits'>Apis <a
					 href='https://www.flickr.com/photos/internetarchivebookimages/14778479881/'>Internet Archive</a></li>
				       
				       <li class='credits'>Bear Claw:
					 <a
					   href='https://www.flickr.com/photos/ucumari/15897351745/'>Valerie</a></li>
				       
				   

				     </ol>
				   </section>
			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
