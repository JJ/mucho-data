<!doctype html>
<html>
	<head>
	  <meta charset="utf-8">
	  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
          
          
          <meta name="description" content="Big data, presentación para Open E2 Arena + IEEE Developer Days">
          <meta name="author" content="JJ Merelo">
          
	  <link rel="stylesheet" href="css/reveal.css">
	  <link rel="stylesheet" href="css/theme/league.css">
          
	  <!-- Theme used for syntax highlighting of code -->
	  <link rel="stylesheet" href="lib/css/zenburn.css">
          
	  <!-- Printing and PDF exports -->
	  <script>
	   var link = document.createElement( 'link' );
	   link.rel = 'stylesheet';
	   link.type = 'text/css';
	   link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
	   document.getElementsByTagName( 'head' )[0].appendChild( link );
	  </script>
	</head>
	<body>
		<div class="reveal">
		  <div class="slides">
                    	<section>
	  <h1>Mucho Data</h1>
	  <h3>Cuando los datos crecen</h3>
	  <p>Por <a
	    href="http://jj.github.io">JJ Merelo</a> / <a
	    href="http://twitter.com/jjmerelo"><code>@jjmerelo</code></a></p>
          
	  <aside class='notes'>Los datos, en general, son una de las
	palabras clave de la tecnología; trabajar con datos es no sólo
	una profesión, sino que empieza a afectar prácticamente a cada
	una de las demás profesiones que tienen que ver con la
	informática. Por ejemplo, los bancos empiezan a convertirse en
	empresas que manejan datos y, además, prestan o almacenan dinero.</aside>
	</section>

	<section
				data-background='img/viaje-datos.jpg'
				alt='autobús NY'>
				  <aside class='notes'>Mucho data es
				un viaje datos, que es como decimos los granaínos. Como en el
				chiste. "¿Quieres un viaje a Túnez?"
				    "¿Para qué quiero yo tanto
				    pescado?"</aside>
	</section>

	<section data-background='https://johnnymackintosh.files.wordpress.com/2011/08/asimov-foundation-covers-cropped.jpg' title='Trilogía de la fundación'>
				  
				  <aside class='notes'>La <a
				href='http://en.wikipedia.org/wiki/Psychohistory_%28fictional%29'>Psicohistoria</a>
				fue propuesta por Asimov en su
				trilogía de la fundación y viene a
				decir que la historia de una masa de
				gente suficientemente grande puede ser
				    predicha, incluso con mucha
				antelación. Pero podía haber ciertas
				desviaciones, como por ejemplo... </aside>
	</section>

				<section data-background='http://sparkleberrysprings.com/v-web/b2/images/entertainment/asimovmule.jpg' title="El mulo">
				  <aside class='notes'>El Mulo no se
				  podía predecir por tal
				  psicohistoria. Y la lió pardísima,
				  claro. La idea del big data no es
				  trabajar con lo que pueda hacer una
				  persona, o un chisme,
				  individualmente, sino... </aside>
				</section>

				<section data-background='https://farm8.staticflickr.com/7133/6863172432_d9cda26a6f_k_d.jpg' title='gente con móviles'>
				  <aside class='notes'>Cuando hay
				  mucha gente, o dispositivos, usando
				  la Internet y produciendo datos es
				  cuando nos encontramos con
				  el <em>big data</em>. La actividad online siempre
				  deja una huella, que, si nos dejan, podemos
				  analizar. Pero el big data no sólo
				  va de la actividad online de la
				  gente; también en física, en
				  finanzas, en ventas. Se ha
				  popularizado el big data de
				  comportamientos online simplemente
				  porque está accesible para el
				  público en general de varias formas
				  posibles. No puedes acceder a las
				  ventas diarias del Mercadona ni a
				  cuanta gente hace streaming de qué
				  serie del canal de series de Movistar, por
				  ejemplo. Sólo puede el señor de
				  Mercadona o don Movistar.
				  </aside>

				</section>

				<section data-background='https://farm5.staticflickr.com/4088/4999986578_5ff5a242a7_b_d.jpg' title='muchas filas y columnas'>
				  <aside class='notes'><em>Big
				  data</em> tiene una definición muy
				  específica: Técnicamente se
				  trata de trabajar con múltiples
				  filas y columnas, como en estas
				  sillas. Todo el tiempo añaden
				  sillas. Se sienta gente. ¿Cuánta
				  gente hay sentada en la 6ª fila?
				    ¿Cuánta gente hay delante
				    mío? ¿Hay algún patrón en las
				  sillas que indique que alguien con
				  la cabeza especialmente grande se ha
				  sentado y no deja ver al resto? El
				  que
				  sea <a href='http://en.wikipedia.org/wiki/Big_data'>big
				  data</a> depende de 
				 las filas, las columnas y del número
				  de las mismas que se añaden cada
				  cierto tiempo.  </aside>
				</section>

				<section data-background='img/uves-ancient-history.jpg'
				  title='Uves'>
				  <aside class='notes'>Se habla de 5
				  uves: velocidad, variedad,
				  veracidad,  volumen y variabilidad,
				  aparte de una C, que es
				    complejidad. Lo que ocurre es
				  que, en general, el común de los
				  mortales, los profes, los que montan
				  apps y webs, no tratan con el
				  big data directamente. Se suele
				  tratar con el resultado de procesar
				  esas técnicas de big data; por eso
				  se habla con más generalidad
				    de <em>ingenieros de datos</em>
				    o <em>científicos de datos</em>. Pero,
				  ¿qué genera tantos datos?</aside>
				</section>
				
				<section data-background='img/colombia-no-es.png'
				  alt='buscando'>
				  <aside class='notes'>Cada vez que
				  buscamos se está generando un dato
				  que además va geolocalizado y por
				  supuesto asignado a nuestro usuario
				  de Google. Cada vez también que
				  encendemos un móvil Android (y
				  supongo que también otros) la
				  localización se almacena y se
				  actualiza cada cierto tiempo si tienes activado
				  Google Now. Cada vez que se visita
				  una página en la Wikipedia también
				  se queda registardo, o una wetb
				  cualquiera (pero en la Wikipedia
				  podemos saberlo). También hay grandes
				  cantidades de datos de origen
				  biológico, procedentes del análisis
				  de genomas, por ejemplo; datos de tráfico, datos
				  de la administración (a veces
				  disponibles como datos abiertos),
				  metadatos de mensajes en sistemas
				  como WhatsApp... uno genera datos
				  sin saberlo por su actividad online,
				  pero también se genera en todo tipo
				  de actividades, sobre todo si la
				  "actividad" es gratuita; recuerda
				  que si un producto es gratis, el
				  producto eres tú. No siempre son ni
				  abiertos ni un viaje de datos, pero pueden
				  llegar a serlo con el tiempo o si
				    los fusionamos con otros. Y ahora
				  queremos ponernos a currar con
				  ellos, así que
				  ¿de dónde sacamos los datos?</aside>

				</section>

				<section>
				  <section
				  data-background='img/el-primo-de-apis.png'
				  title='Primo japonés de Apis'>
				    <aside class='notes'>
				      <p>Aquí tenía que venir una
					transpa de APIs, el paté, ya sabéis,
					Apis, Papis. Era lo que iba buscando
					pero me encontré una historia que,
				  curiosamente, tiene que ver con el
					<em>big data</em> y la <em>big family</em>. Buscando
				  imágenes para ilustrar esta
				  transparencia, no encontré ninguna
				  buena de patés Apis. Así que, no reparando en
				  esfuerzos para tener una
				  transparencia chachi, me fui al
				  Mercadona de al lado de Informática a buscar paté Apis,
				  tomate triturado Apis o lo que
				  fuera (Apis). No lo encontré (y saqué esta
				  foto en vez de la anterior). Pero
				  buscando la razón por la que un
				  elemento tan fundamental de las
				  meriendas patrias no estuviera en el Mercadona, me
				      encontré con <a
				  href='http://blogs.elconfidencial.com/espana/el-confidente/2009-02-13/el-clan-familiar-ruiz-mateos-se-moviliza-para-ir-a-comprar-apis-a-mercadona_441793/'>esta
				  noticia</a> que habla de como la
				      <em>big family</em> de Ruiz
				  Mateos trató de engañar a los
				      algoritmos <em>big data</em> de
				  Mercadona mandando a miembros de la
				  familia extensa a pedir o comprar
				  sus productos o reclamarlos en caso
				  de que no lo encontraran al
				  enterarse que Mercadona iba a
				  rescindir el contrato con ellos. Lo que
				  ocurre es que trataron de difundir
				  la campaña por correo electrónico y,
				  por equivocación, lo enviaron a la
				  propia Mercadona. Algo muy común en
				      las <em>big family</em>, que
				  siempre hay una prima que se llama
				  Mercadona o trabaja en el Mercadona
				  o que se llama Mercedes pero, por su
				  afición a los productos Hacendado,
				      le llaman Mercedona. O algo.</p>

				      <p>Los denominados
				  APIs, o interfaces de programación de
				  aplicaciones (<em>application
					  programming interface</em>), permiten extraer
				  información ya procesada sobre la
				  actividad online en una serie de
				  servicios en la web. Por ejemplo, los APIs permiten
				  recuperar tweets en Twitter (es lo
				  que las apps de Twitter como
				  Tweetdeck o Hootsuite), likes
				  en Facebook, visitas en la Wikipedia
				  y una gran cantidad de cosas sobre
				  la actividad pública de la
				  gente. También se puede acceder a
				  otro tipo de APIs, recordad que el
				  big data no se refiere sólo y exclusivamente a la gente:
				  por
				  ejemplo <a href='http://en.wikipedia.org/wiki/List_of_financial_data_feeds'>datos
				      de acciones en bolsa</a>
				  o <a href='https://code.google.com/p/googletransitdatafeed/wiki/PublicFeeds'>datos
				      de tráfico</a>
				  o <a href='http://stats.stackexchange.com/questions/12670/data-apis-feeds-available-as-packages-in-r'>datos
				      de todo tipo, desde el tiempo
				  hasta archivos</a>. Por cierto,
					vemos por primera vez <a href='http://r-project.org'>R</a>, el lenguaje
				  que se usa, por excelencia, para
				  trabajar con los datos. Lo veremos
					más adelante.</p>
				    </aside>

				  </section>
				  
				  <section>
				    <img src='img/cun-vs-un-co.png' alt='Visitas a la página de la wikipedia'>
				    <aside class='notes'><p>Por ejemplo,
				    usando un API muy simple de la
				    Wikipedia se pueden descargar las
				    visitas a las páginas día a
				    día. Aquí las visitas a la página
				    de la corporación universitaria
				    nacional y de la universidad nacional.</p>

				    <p>De hecho, la Wikipedia da
				  acceso a <a
				  href='https://wikitech.wikimedia.org/wiki/Analytics'>una
				  buena cantidad de datos de una forma
				      sencilla</a>. Viene a ser una de las
				  banderas de la nueva web (que ya no
				  sé si va por la dos o la tres o ya
				  ni siquiera se llama web): se crean
				  plataformas, no sólo sitios con
				  contenido, y las páginas son fuentes
				  de datos sin estructura a los que
				  también se puede acceder mediante APIs. También es un signo de
				  transparencia: por eso se puede
				  acceder de esa forma a los datos de
				  la Wikipedia (y no tanto a los de
				      FB, por ejemplo).</p></aside>
				  </section>
				  
				  <section>
				    <pre><code>rawData.us <- getURL("https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/es.wikipedia/all-access/all-agents/Corporaci%C3%B3n_Unificada_Nacional_de_Educaci%C3%B3n_Superior/monthly/2015100100/201810100")

data.CUN <- fromJSON(rawData.us)
df.CUN <- data.frame(matrix(unlist(data.CUN),nrow=36, byrow=T))
df.CUN$Views <-   as.numeric(levels(df.CUN$X7))[df.CUN$X7]
df.CUN$Date <-  as.Date(df.CUN$X4,"%Y%m%d00")

ggplot(data=df.CUN,aes(x=Date,y=Views,group=1,col='CUN'))+geom_line()+geom_line(data=df.UN,aes(x=Date,y=Views,group=1,col='UN'))

</code></pre>

				    <aside class='notes'>Es sólo parte
        del programa, que se puede descargar <a
				      href='datos/univs-co.R'>de
				    aquí</a>; también se puede
				    consultar la historia de R, es
        decir las pruebas, los datos y el resto de los comandos
        ejecutados. Recordad, todo software libre. En
				    cualquier caso, si no entendéis
				    del tema no tenéis que
				    preocuparos: está escrito en un
				    lenguaje llamado R, que es un
				    lenguaje que os vais a encontrar
				    una y otra vez cuando se habla de
				    ciencia de datos y de big
				    data. La idea es que los APIs o
				      interfaces de programación
				    <em>publican</em> los datos en un
				    formato estándar, que se pueden
				    usar desde cualquier lenguaje de
				    programación, en particular este
				    R. </aside>
				  </section>
				</section>

				<section>
				  
				  <section
				    data-background='https://farm8.staticflickr.com/7467/15897351745_ccc1232f4e_h_d.jpg'
				     title='Rascando/scraping'>
				    <aside class='notes'>Pero no todo
				      el mundo es tan fácil y
				      accesible. <em>Scraping</em> es
				      extraer datos de dónde estén
				      publicados sin, en principio,
				      intención de que se descarguen. 
				    </aside>
				  </section>

				  <section>
				    <h2>¿Cuánto mide el Millonarios?</h2>
				    <img src='img/millonarios-alturas.png'
				    alt='Altura de los jugadores del
				    Millonarios'>

				    <aside class='notes'>Los pesos se
				    han extraído de la página oficial
				    del Millonarios y no ha sido nada
				    fácil, y de hecho faltan bstantes
				    la transparencia más complicada
				      (después de la del
				    paté; al menos en este no tuve que
				    ir al Mercadona y de camino
				    comprar plátanos). Además, lo
				    hemos ilustrado con la posición
				    que ocupan.</aside>
				  </section>

				  <section>
				    <pre><code>my $url = "http://www.millonarios.com.co/team";

my $dom = Mojo::DOM->new( get $url );
my @pesos;
my $players = $dom->find("select#player option[value]");
# ...
    my $this_dom = Mojo::DOM->new( get "http://www.millonarios.com.co/".$p->val );
    my $jugador = $this_dom->at("div.sombra-detalle");
    my $nombre = $jugador->find("strong")->map("text")->join( " " );
    my $data = $jugador->find("p")->map("text");
    my ($posicion) = ($data->[0] =~ /(\w+),/);
    my ($peso) = ($data->[3] =~ /(\d+)/);</code></pre>

				    <aside class='notes'>Seguro que la
				    mayoría no entiende nada. Si no
				    entendéis nada, no miréis. Si R es
				    complicado, este lenguaje, llamado
				    Perl, ya es la leche. ¿Alguien lo
				    ha usado? Pues debería, porque
				      hacer esto, <em>rascar</em>
				      información de una página es
				    <em>relativamente</em> (muy
				    relativamente, de hecho),
				    simple. También se puede hacer en
				    R, por cierto, pero no es tan
				    fácil (o si lo es, no tengo ni
				    idea de cómo hacerlo; cada uno usa
				    las herramientas que le resultan
				    más familiares). Perl es software
				    libre y todas las librerías, o
				    colecciones de procedimientos para
				    hacer este tipo de cosas, son
				    también software libre.</aside>
				  </section>

				  <section
				  data-background='https://farm3.staticflickr.com/2811/9825503886_4b9bf98a83_o_d.jpg'
				  title='Numbers'>

				    <aside class='notes'>Hace falta un
				    verdadero detective para extraer
				    los datos. Un detective como
				    Batman... La
				    información qeu hay en una página
				    web está semiestructurada, es
				    decir, tiene una cierta
				    estructura, pero a veces no es
				    totalmente regular o no se puede
				    distinguir a simple vista. Los números
				    están semiocultos, muchas veces
				    simplemente por tener haber hecho
				    la página web a mano, o
				    semiautomáticamente con algún
				    retoque, otras veces por empeño
				    explícito para ocultar datos, como
				    suele suceder en ciertos
				    servicios, empresas o
				    ayuntamientos. La cuestión es que
				    si está en la web y tú lo puedes
				    ver, en la mayoría de las veces,
				      lo puede <em>extraer</em> un
				      programa, así que
				    <em>scraping</em> es siempre una
				    solución de último
				    recurso.</aside>
				  </section>

				<section>
				  <h1>Estoy harto de tanto rascar</h1>
				  <aside class='notes'>Hay
				  herramientas para trabajar con
				  diferentes conjuntos de datos
				  automáticamente e incluso un sitio,
				    <a
				  href='http://morph.io'><code>morph.io</code></a>
				  para almacenar procedimientos de
				  scrapeado y ejecutarlos
				  automáticamente; todo ello en
				  scripts también libres que puedes
				  modificar y adaptar a las
				  necesidades propias. También hay <a
				  href='http://michelleminkoff.com/outwit-needlebase-hands-on-lab/'>extensiones
				      a Firefox como Outwit</a>. Hay
				  que trabajar de <a
				      href='http://en.wikipedia.org/wiki/Web_scraping'>múltiples
				  formas</a> pero, en muchos casos, se
				  pueden obtener datos suficientes
				    como para trabajar. Para hacer
				  scraping en profundidad, en general,
				  hay que tener ciertos conocimientos
				  de programación web y saber, al
				  menos, leer la estructura del fuente
				  de una página web, porque lo que se
				  hace, en la mayor parte de las
				  ocasiones, es eso. En algunos casos
				  también es cuestión de copiar y
				  pegar, pero si tienes que actualizar
				  la información cada cierto tiempo
				  puede acabar siendo muy tedioso. Más
				  recursos, incluyendo <a
				  href='https://zenagiwa.wordpress.com/2014/10/11/non-programmers-guide-to-scraping-data/'>extensiones
				      para Chrome, están en esta
				  página</a>. Incluso si usáis <a
				  href='http://www.labnol.org/internet/google-web-scraping/28450/'>la
				  hoja de cálculo de Google Drive y
				  con un poquito de programación
				  puedes trabajar hasta con cosas que
				      se actualicen periódicamente.</a></aside>
				</section>

				<section
				data-background='https://farm4.staticflickr.com/3129/3206624177_eeea3622ff_b_d.jpg'
				title='los números son chungos'>

				  <aside class='notes'>A veces la
				  forma mejor de ocultar algo es
				  publicarlo. No todo lo publicado es
				  transparente: los PDFs, por ejemplo,
				  son bastante difíciles de consultar:
				  una tabla en PDF es un reto a veces
				  insuperable. Y en algunos casos los
				  PDFs son documentos escaneados,
				  mucho más difíciles todavía de
				  extraer. En resumen: el reto en los
				  datos, en muchas ocasiones y sobre
				  todo en el periodismo de datos, es
				    poder extraerlos.</aside>
				  </aside>
				</section>

				</section>

				<section
				    data-background='https://farm1.staticflickr.com/108/283959170_7885d48f3d_o_d.jpg'
				    title='Números en contenedores'>
				    <aside class='notes'>Vale, ya
				    tenemos todos los numeritos. Pero,
				      ¿qué podemos hacer con los datos
				    si son verdaderamente grandes?</aside>
				</section>

				<section
				data-background='img/paro-elche.png'>
				  <aside class='notes'>Lo primero es
				visualizarlos. Por ejemplo, <a
				href='http://espanaencifras.elespanol.com/?utm_content=bufferee543&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer#home_maps'>estos
				son los datos de variación del paro en
				    Elche y comarca desde 2011</a>,
				    hecho con una herramienta llamada
				<a
				href='http://cartodb.com'>CartoDB</a>,
				software libre hecho por una empresa
				española. Esta aplicación, aparte de
				poder instalártela en tu casa, permite
				subir datos geolocalizados (o
				geolocalizarlos fácilmente) y
				representar de decenas de formas
				diferentes sobre mapas de todo tipo. </aside>
				</section>

				<section>
				  <section data-background='img/presupuestos-cv.png'>
				    <aside class='notes'>En <a
				      href='http://www.dondevanmisimpuestos.es/ccaa/'>Donde
				      Van mis Impuestos</a> visualizan
				      los impuestos de las comunidades
				      autónomas, los globales y los de los
				      ayuntamientos, mostrando dónde van
				      las diferentes partidas y análisis
				      como el presupuesto por
				  persona. Donde van mis impuestos es
				  un proyecto de la <a
				  href='http://www.civio.es/'>Fundación
				      Ciudadana Civio</a>, que tiene
				  otros muchos proyectos relacionados
				  con la transparencia. </aside>
				  </section>

				  <section
				    data-background='img/presupuestos-ugr.png'
				    title='presupuestos de la UGR'>
				    <aside class='notes'>Estos
				  presupuestos fueron extraídos por <a
				  href='http://openkratio.github.io/ugr-presupuestos/'>Félix
				      Ontañón en 2013</a>, además de
				  un PDF, que tiene mucho más
				  mérito. La visualización es todo un
				  arte y contribuye a hacer evidentes
				  relaciones entre cantidades,
				  patrones que se pueden captar a
				  simple vista. Hay empresas como
				  Vizzuality (de donde salió CartoDB)
				      y otra como <a
				  href='http://graphext.com/'>Graphext</a>,
				  creada hace poco por un granadino y
				      afincada en Alicante.</aside>
				  </section>

				</section>
				
				<section data-background='https://farm6.staticflickr.com/5508/11672279656_0660ccd442_k_d.jpg'
				title='la masa'>
				  <aside class='notes'>Una de las
				  opciones es machacar los datos, y si
				  los datos no son muy grandes, o bien
				  ya están preprocesados de alguna
				  forma, podemos hacerlo
				    usando <em>técnicas
				  tradicionales</em>: bases de datos,
				  un sólo ordenador, cosas
				  así. Normalmente, sin embargo,
				    necesitaremos algo un poco más
				potente. Y aquí es donde entra... El
				científico de datos. </aside>
				</section>

				<section>
				  <h2>Esto se parece a...</h2>
				  <img
				  src='img/drowns-vs-nicholas-cage.png'
				  title='Ahogados vs. Pelis de
				  Nicholas Cage'>
				  <aside class='notes'>Hay tantas
				  posibilidades de comparar datos con
				  datos que hasta hay una <a
				  href='http://www.tylervigen.com/spurious-correlations'>página
				    web dedicada a correlaciones
				  espurias</a>. Una vez que se tienen
				  datos, se pueden comparar con
				    cualquier cosa. Se trata de buscar
				  <em>correlaciones</em>, para lo cual
				  hacer gráficos no es suficiente: hay
				  que descartar cualquier otro factor
				  y también usar herramientas
				  estadísticas para probar sin lugar a
				  dudas que existe esa correlación y
				  que un factor depende de
				  otro. <a
				  href='http://stats.stackexchange.com/questions/36/examples-for-teaching-correlation-does-not-mean-causation'>Correlación
				    no implica causación</a>, como lo
				  demuestra el hecho de que el número
				  de premios nóbeles está
				  correlacionado con el consumo de
				  chocolate. </aside>
				</section>

				<section
				data-background='https://farm2.staticflickr.com/1148/1403196887_c7d1e216aa_b_d.jpg'
				title='torrentes de datos'>
				  <aside class='notes'>En realidad,
				nos hemos saltado una parte importante
				    del <em>big data</em>, o lo que lo es
				verdaderamente. Procesamiento de
				grandes cantidades de datos, con las 5
				Vs y la V más importante, la velocidad
				y el volumen, como este torrente
				callejero en Teherán. Es algo que tu
				ordenador solitario no va a poder
				    procesar. En general, necesitarás
				la nube.</aside>
				</section>

				<section
				data-background='https://farm4.staticflickr.com/3507/3887649008_8cf9e7027e_b_d.jpg'
				title='Nubes sobre Trondheim, foto
				mía'>
				  <aside class='notes'>En general,
				hace falta trabajar con la nube. La
				nube, para lo que nos interesa, es una
				infraestructura de computación que es
				escalable y que se paga por
				uso, compuesta por máquinas virtuales
				con algún tipo de software
				ejecutándose (libre, claro) y
				configuradas también por
				software. Hay cloud de unos cuantos
				"players" que incluye a Amazon, Google,
				Microsoft. La nube se adapta a la
				necesidad que cada persona
				tenga. Pero, a este nivel, simplemente
				estamos ejecutando una aplicación (con
				equilibrado o lo que sea) en una
				"máquina" que es elástica (o nubosa,
				que se nos va la
				metáfora). Generalmente hace falta
				    algún tipo de procesamiento
				específico.</aside>

				</section>

				<section>
				  <section
				    data-background='https://farm4.staticflickr.com/3196/3047558085_c2e7e88386_b_d.jpg'
				  title='map reduced'>

				    <aside class='notes'>Posiblemente el
				      peor chiste de la charla: mapa
				      reducido, o <em>map reduce</em>,
				      una de las técnicas principales
				      aplicadas en <em>big data</em>
				para procesar grandes cantidades de
				datos. Consiste en un procesamiento en
				    dos fases</aside>
				  
				  </section>

				  <section>
				  <img
				  src='http://upload.wikimedia.org/wikipedia/commons/9/9a/Mapreduce_%28Ville_Tuulos%29.png' style='height:600px'
				  title='map-reduce'>

				  <aside class='notes'>Los datos son
				    aplicados (que es lo que significa
				    map) es decir, filtrados, o extraídos,
				    ordenados, a una serie de resultados
				    intermedios, que más adelante son
				    agregados (por ejemplo, sumados
				    siguiendo algún criterio) o reducidos
				    de forma que den unos datos que son
				    los que, eventualmente se van a
				    utilizar. Google aplica continuamente
				    este tipo de procesamiento para
				    presentar las tendencias, para dar el
				    número de resultados de una consulta y
				    para cosas similares.</aside>
				  </section>

				  <section>
				    <h2>MapReduce es Hadoop</h2>
				    <img src='http://upload.wikimedia.org/wikipedia/commons/0/0e/Hadoop_logo.svg' title='"Hadoop logo" by
				  Apache Software Foundation -
				  https://svn.apache.org/repos/asf/hadoop/logos/out_rgb/. Licensed
				  under Apache License 2.0 via
				  Wikimedia Commons -
				  http://commons.wikimedia.org/wiki/File:Hadoop_logo.svg#/media/File:Hadoop_logo.svg'>

				    <aside class='notes'><a
				    href='http://en.wikipedia.org/wiki/Apache_Hadoop'>Hadoop
				      es un programa</a> de la
				    fundación Apache (la misma del
				    servidor web) y desarrollado
				    originalmente por Yahoo, escrito
				    en Java y que implementa Map
				    reduce sobre un grupo de
				    ordenadores físicos o máquinas
				    virtuales. Normalmente no tiene
				    que preocuparse uno de montarlo:
				    si usas algún recurso en nube ya
				    viene montado automáticamente y no
				    hay más que usarlo, creando
				    programas. Casi todas las grandes
				    empresas de Internet, desde
				    Facebook a Google, usan este tipo
				    de programa, quizás adaptado. Y en
				      la nube es fácil
				    implementarlo. Últimamente se está
				    empezando también a hablar de otra
				    implementación, Spark, que es
				    mucho más rápida que Hadoop
				    (aunque está basada en él) y que
				      está hecha en otro lenguaje, <a
				    href='http://datascience.stackexchange.com/questions/441/what-are-the-use-cases-for-apache-spark-vs-hadoop'>Scala</a>. En
				    general, este mundo de los datos
				    se mueve a una velocidad que es
				    imposible mantenerse al día. Y
				      luego tienes <a
				    href='https://pig.apache.org/'>Pig</a>,
				    un lenguaje para analizar
				      conjuntos de datos, y <a
				    href='http://stackoverflow.com/questions/3356259/difference-between-pig-and-hive-why-have-both'>Hive</a>
				    en caso de que quieras usar datos
				    estructurados. En resumen: hay
				    muchas herramientas para asistir
				    al científico de datos.</aside>
				  </section>

				</section>

				<section data-background='img/machine-learning.jpg'>
				  <aside class='notes'>Hay que ir un
				  poco más allá buscando patrones,
				  aprendiendo cosas y prediciendo qué
				  va a suceder a continuación. Cuando
				  trabajas con mucho data, tela de
				  data, un viaje de data, los
				  algoritmos que se utilizan para
				  aprendizaje tienen que ir sobre la
				  infraestructura anterior. Por
				  ejemplo, <a
				  href='http://mahout.apache.org/users/basics/algorithms.html'>Mahout
				    funciona sobre Spark</a> y tiene
				  todo tipo de algoritmos de
				  clustering, clasificación y de
				    filtrado colaborativo. Y todo esto, ¿para qué sirve?</aside>
				</section>

				<section>
				  <section>
				    <h2>¿Qué está pasando ahora mismo?</h2>
				    <h1 class='fragment'>Now-casting</h1>
				    
				    <aside class='notes'>Una tienda de
				      ropa tiene que saber desde hace
				      varios días qué es lo qeu tiene que
				      haber en la tienda <em>hoy</em>;
				      una fábrica necesita predecir los
				      pedidos para ver de dónde van a
				      venir. Y además, necesita saber de
				      dónde le van a venir.</aside>
				  </section>

				  <section>
				    <img src='img/nowcasting.png'
				    title='tendencias en zapatos'>
				    <aside class='notes'>Por ejemplo,
				    ¿qué tipo de zapatos va a pedir la
				    gente? ¿Sandalias? ¿Chanclas?
				    ¿Náuticos?
				    </aside>
				  </section>

				</section>

				<section
				    data-background='https://farm5.staticflickr.com/4146/4984385396_dd18583007_b_d.jpg'
				    title='historias alrededor del fuego'>
				    <aside class='notes'>Lo importante
				    con los datos es que cuenten una
				    historia. Una predicción, un mapa,
				    una gráfica de correlación, no
				    significa absolutamente
				    nada. ¿Existe alguna causa? ¿Cómo
				    puedes explicar lo que ocurre? Si
				    descubres que la cantidad de
				    crímenes está correlacionada con
				    la venta de helados, ¿por qué
				    puede ser? ¿Cómo puedes transmitir
				    lo que los datos te están
				    contando? 
				    </aside>
				</section>

				<section>
				  <h2>Hay <em>mucho data</em></h2>
				  <h1 class='fragment'>Pongamos
				    nosotros la historia</h1>

				  <aside class='notes'>La
				psicohistoria de Asimov va a estar
				jodida. Pero al menos podremos
				entender un poquito mejor nuestro
				mundo y tomar decisiones más
				    informadas.</aside>
				</section>

				<section style="text-align: left;">
					<h1>¿Alguna pregunta?</h1>
				</section>

				 <section>
				   <section>
				     <h2>Créditos</h2>
				     
				     <ol>
				       <li class='credits'>Imagen del
					 bus modificada de <a
					 href='https://www.flickr.com/photos/mtaphotos/10004448523/'>MTA photos</a></li>
				       <li class='credits'>Imagen de la Fundación del <a href='http://johnnymackintosh.com/2011/08/23/influences-on-johnny-mackintosh-isaac-asimov/'>blog de Johnny Mackintosh</a></li>
				       <li class='credits'>Imagen del mulo de <a href='http://www.jacurutu.com/viewtopic.php?f=21&t=3213'>un foro</a></li>
				       <li class='credits'>Imagen de
					 gente con móviles
					 de <a href='https://www.flickr.com/photos/kk/6863172432/'>Kris
					 Krüg</a></li>
				       <li class='credits'>Imagen de
					 fila de sillas
					 por <a href='https://www.flickr.com/photos/2_funky/4999986578/'>2 funky</a></li>
				       <li class='credits'>Uves
					 de <a href='https://www.flickr.com/photos/ancienthistory/11244652695/'>ancient history</a></li>
				       
				       <li class='credits'>Hulk
					 de <a
					 href='https://www.flickr.com/photos/clement127/11672279656/'>Clement127</a></li>
				       
				       <li class='credits'>Zarpa de oso de
					 <a
					   href='https://www.flickr.com/photos/ucumari/15897351745/'>Valerie</a></li>
				       
				   

				     </ol>
				   </section>
				   <section>  <h2>Créditos II</h2>
				     
				     <ol>

				           <li class='credits'>Números de <a
					 href='https://www.flickr.com/photos/andymag/9825503886/'>Andi
					 Maguire</a></li>

				   <li class='credits'><a
				 href='https://www.flickr.com/photos/stuartpilbrow/3206624177/'>Números
				     de Perdidos por
				 StuartPilbrow</a></li>

				   <li class='credits'><a
				 href='https://www.flickr.com/photos/bocadorada/283959170/'>Tuppers
				     de Boca Dorada</a></li>

				   <li class='credits'><a
				     href='https://www.flickr.com/photos/hamed/1403196887/'>Torrente
				     por Hamed Saber</a></li>
				   
				   <li class='credits'><a
				     href='https://www.flickr.com/photos/george/3047558085/'>Mapa
				     doblado de George Oates</a></li>
				   <li class='credits'><a
				     href='https://www.flickr.com/photos/jkirkhart35/4984385396/'>Fuego
				     de campamento de Jerry Kirkhart</a></li>
				   
				 </section>

                                 
		  </div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
		 Reveal.initialize({
                  controls: true,
		  progress: true,
		  history: true,
		  center: true,
		  transition: 'slide', // none/fade/slide/convex/concave/zoom

		 dependencies: [
		   { src: 'plugin/notes/notes.js', async: true },
		   { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
		 ]
		 });
		</script>
	</body>
</html>
